## Model 정리


#### kNN (K-Nearest Neighbors, 최근접 이웃 알고리즘)
: 예측하고 싶은 대상 주변을 확인하여 주변 객체의 개수에 따라 가장 많은 객체가 포함되어 있는 것으로 예측하는 방법

- K는 최근접 점(객체)을 몇 개까지 볼 것인지에 대한 변수
- 보통 K는 작고 홀수를 쓴다 ( 이유 : 짝수를 쓰게 되면 2대2 같은 상황이 나오게 되면 답을 할 수 없는 상황도 나오기 때문이다.
- 근접 측정 방법은 거리계산 (피타고라스)

#### Decision Tree (의사결정트리)
: yes 또는 no의 답변을 할 수 있도록 질문하고 여러 질문을 통해 원하는 결과값을 만들어 학습시킨 뒤 이후에 같은 방법으로 결과값을 예측하는 방법

- 문제를 제시하여 학습데이터를 넣고 -> 의사결정트리를 만든 뒤에 -> 학습을 하고 -> 그 의사결정트리에 test데이터를 넣어 테스트
- 질문 순서 선정 방법은 최대한 많은 데이터를 걸러낼 수 있는 질문부터 선정하게 됨
- Entropy : 높은 엔트로피는 어지럽고 낮은 엔트로피는 깔끔함 (여러 질문을 통해 점점 엔트로피가 낮아지게 됨)
- Information Gain : base entropy - new entropy 인포메이션 겐이 가장 높은 걸로 다음 attribute(질문)

#### Naive Bayes (나이브 베이즈 분류)
: 베이즈안 정리를 이용하여 찾고자 하는 경우가 어떤 경우에 많이 일어나는지 확률적으로 예측하는 방법
  
- Conditional Probability (조건부확률) : 어떠한 상황이 일어났을 때 그 상황 속에서 다른 상황이 일어날 확률
- ex) 스팸메엘 분류 : free라는 단어가 들어가 있는 메일 중 스팸일 확률, free와 coupon이라는 단어가 들어가 있는 메일 중 스팸일 확률 비교 ...

#### Linear regression (선형회귀)
: 학습을 통해 선을 만들어 내고 그 선을 통해 새로운 데이터가 들어왔을 때 함수에 새로운 데이터의 변수를 대입하여 값을 예측

- square errors : 그려진 선과 원래 결과값의 차이를 error라 하고 그 error를 제곱한 값
  - 제곱을 쓰는 이유? 1. 눈으로 확인하기 쉬움 2. 에러가 작아도 값을 증폭시켜 큰 차이와 작은 차이 비교 쉬움 3. Gradient descent에서 제곱시에 계산 용이
- mean square error : square errors 값들의 평균 (= cost function)
- Least Mean Square (LMS) Error 를 찾아야함
- converge될 때 = gradient 값이 대략 0이 될 때

#### k-mean clustering
: 
